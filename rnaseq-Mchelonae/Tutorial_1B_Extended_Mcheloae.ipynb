{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extended RNA-Seq Analysis Training Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity and time, The short tutorial workflow uses truncated and partial run data from the Cushman et al., project.\n",
    "\n",
    "The tutorial repeats the short tutorial, but with the full fastq files and includes some extra steps, such as how to download and prepare the transcriptome files used by salmon, alternate ways to navigate the NCBI databases for annotation or reference files you might need, and how to combine salmon outputs at the end into a single genecount file.\n",
    "\n",
    "Full fastq files can be rather large, and so the downloading, extracting, and analysis of them means this tutorial can take over 1 hour 45 minutes to run the code fully. This is part of the reason we have a short and easy introductory tutorial, and this longer more full tutorial for those interested.\n",
    "\n",
    "If this is too lengthy feel free to move on to the snakemake tutorial or the DEG analysis tutorial -- all the files used in the DEG tutorial were created using this extended tutorial workflow.\n",
    "\n",
    "![RNA-Seq workflow](images/rnaseq-workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Install Mambaforge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First install Mambaforge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download Miniforge or Mambaforge (you can use either based on preference)\n",
    "!curl -L -O https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh > /dev/null 2>&1\n",
    "\n",
    "# Install Miniforge (or Mambaforge) - no need to install conda since mamba will be available immediately\n",
    "!bash Miniforge3-$(uname)-$(uname -m).sh -b -u -p $HOME/miniforge > /dev/null\n",
    "!date +\"%T\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, using mambaforge and bioconda, install the tools that will be used in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tell the computer where the mambaforge bin files are located\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + os.environ[\"HOME\"]+\"/mambaforge/bin\"\n",
    "\n",
    "#now we can easily use 'mamba' command to install software \n",
    "!mamba install -y -c conda-forge -c bioconda trimmomatic fastqc multiqc salmon gsutil sql-magic entrez-direct gffread parallel-fastq-dump sra-tools sql-magic pyathena -y > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a set of directories to store the reads, reference sequence files, and output files. Notice that first we remove the `data` directory to clean up files from Tutorial_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cd $HOMEDIR\n",
    "! echo $PWD\n",
    "! rm -r data/\n",
    "! mkdir -p data\n",
    "! mkdir -p data/raw_fastq\n",
    "! mkdir -p data/trimmed\n",
    "! mkdir -p data/fastqc\n",
    "! mkdir -p data/aligned\n",
    "! mkdir -p data/reference\n",
    "! mkdir -p data/fastqc_samples\n",
    "! mkdir -p data/multiqc_samples\n",
    "! mkdir -p data/quants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set # THREADS depending on your VM size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numthreads=!lscpu | grep '^CPU(s)'| awk '{print $2-1}'\n",
    "\n",
    "#python variable to hold the amount of threads your cpu has,\n",
    "#useful for downstream tools like salmon, trimmomatic, etc\n",
    "threads = int(numthreads[0])\n",
    "\n",
    "#its also good to have a shell version of the variable for commands that use piping, \n",
    "#in jupyter, shell commandds with piping sometimes causes python variables to not work and generally be wonky.\n",
    "%env THREADS=$threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 3: Downloading relevant FASTQ files using SRA Tools\n",
    "\n",
    "Next we will need to download the relevant fastq files.\n",
    "\n",
    "Because these files can be large, the process of downloading and extracting fastq files can be quite lengthy.\n",
    "\n",
    "The sequence data for this tutorial comes from work by Cushman et al., <em><a href='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8191103/'>Increased whiB7 expression and antibiotic resistance in Mycobacterium chelonae carrying two prophages</a><em>.\n",
    "\n",
    "We will be downloading the sample runs from this project using SRA tools, downloading from the NCBI's SRA (Sequence Run Archives).\n",
    "\n",
    "However, first we need to find the associated accession numbers in order to download.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.1: Finding run accession numbers.\n",
    "\n",
    "The SRA stores sequence data in terms of runs, (SRR stands for Sequence Read Run). To download runs, we will need the accession ID for each run we wish to download. \n",
    "\n",
    "The Cushman et al., project contains 12 runs. To make it easier, these are the run IDs associated with this project:\n",
    "\n",
    "+ SRR13349122\n",
    "+ SRR13349123\n",
    "+ SRR13349124\n",
    "+ SRR13349125\n",
    "+ SRR13349126\n",
    "+ SRR13349127\n",
    "+ SRR13349128\n",
    "+ SRR13349129\n",
    "+ SRR13349130\n",
    "+ SRR13349131\n",
    "+ SRR13349132\n",
    "+ SRR13349133\n",
    "\n",
    "\n",
    "In this case, all these runs belong to the SRP (Sequence Run Project): SRP300216.\n",
    "\n",
    "Sequence run experiments can be searched for using the SRA database on the NCBI website; and article-specific sample run information can be found in the supplementary section of that article.\n",
    "\n",
    "For instance, here, the the authors posted a link to the sequence data GSE (Gene Series number), <a href='https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE164210'>GSE164210</a>. This leads to the appropriate 'Gene Expression Omnibus' page where, among other useful files and information, the relevant SRA database link can be found. \n",
    "\n",
    "Once the accession numbers are located, one can make a text file containing the list of accession IDs however they like.\n",
    "\n",
    "Once again, to make things easier, we have made a .txt with these IDs that you can simply download here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://rnaseq-myco-bucket/reference/accs.txt .\n",
    "!cat accs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can can also use BigQuery to generate an accession list following the instructions outlined in [this notebook](https://github.com/STRIDES/NIHCloudLabGCP/blob/main/tutorials/notebooks/SRADownload/SRA-Download.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.1.2 (Optional): Generate the accession list file with BigQuery\n",
    "This step uses Python. We will use the BigQuery API. \n",
    "\n",
    "We will create a client the using default project. \n",
    "\n",
    "Then we will query BigQuery using the species name and a range of accession numbers associated with this particular study. \n",
    "\n",
    "Feel free to play around with the query to generate different variations of accession numbers!\n",
    "\n",
    "Please note that if you have errors to make sure you have this API enabled. You can search for BigQuery by navigating back to the Google Cloud Platform dashboard, back to the Google Cloud Platform, and using the search bar at the top, search for 'BigQuery'. On the BigQuery page, click  `Enable`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "aws sts get-caller-identity\n",
    "### STEP 3.2: Using the SRA-toolkit for a single sample.\n",
    "\n",
    "Sequence run accession IDs can be used to download sequence data, using the 'prefetch' tool of the SRA-toolkit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "# Use the correct argument name: s3_staging_dir\n",
    "conn = connect(s3_staging_dir='s3://sra-data-athena/', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the Glue client\n",
    "glue_client = boto3.client('glue', region_name='us-east-1')\n",
    "\n",
    "# Run the crawler\n",
    "crawler_name = 'sra_crawler'  # Use your crawler's name\n",
    "glue_client.start_crawler(Name=crawler_name)\n",
    "\n",
    "print(f\"Crawler {crawler_name} started.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM AwsDataCatalog.srametadata.metadata\n",
    "WHERE organism = 'Mycobacteroides chelonae' \n",
    "AND acc LIKE '%SRR133491%'\n",
    "\"\"\"\n",
    "df = pd.read_sql(\n",
    "    query, conn\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#write the SRR column to a text file\n",
    "with open('accs.txt', 'w') as f:\n",
    "    accs = df['acc'].to_string(header=False, index=False)\n",
    "    f.write(accs)\n",
    "    \n",
    "#print the text file\n",
    "!cat accs.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#the 'prefetch' command downloads an SRA file.\n",
    "!prefetch SRR13349123 -O data/raw_fastq -f yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Here we can see the command for downloading a single SRA file using an acecssion ID 'SRR13349123'\n",
    "\n",
    "Notice the SRA archives sequence files in the SRA format. \n",
    "Typically genome workflows process data in the form of zipped or unzipped .fastq, or .fasta files\n",
    "So before we move on, we need to convert the files from .sra to .fastq.\n",
    "\n",
    "There are multiple ways to do this. \n",
    "\n",
    "Included in the sra toolskit are fastq-dump and fasterq-dump. These convert SRA to FASTQ.\n",
    "\n",
    "If you use fasterq-dump, its recommended to zip your fastq files after they are created.\n",
    "\n",
    "There is also a tool called 'parallel-fastq-dump' which supports zipping the fastq files automatically into fastq.gz files.\n",
    "\n",
    "The below code may take approximately 15 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#convert sra to fastq\n",
    "!fasterq-dump data/raw_fastq/SRR13349123 -f -O data/raw_fastq/\n",
    "#compress fastq to fastq.gz to save space\n",
    "!gzip data/raw_fastq/SRR13349123_1.fastq\n",
    "!gzip data/raw_fastq/SRR13349123_2.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 3.3 Downloading multiple files using the SRA-toolkit.\n",
    "\n",
    "Often one wants to, as in our case, wish to download multiple runs at once.\n",
    "\n",
    "To aid in this, SRA-tools supports batch downloading. This is why we created the text file earlier.\n",
    "\n",
    "We can download multiple SRA files using a single line of code by using our list SRA IDs, and inputting that into the prefetch command.\n",
    "\n",
    "And then feed that list into the sra-toolkit prefetch command. Note, it may take some time to download all the fastq files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!prefetch --option-file accs.txt -O data/raw_fastq -f yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3.4 Converting Multiple SRA files to Fastq\n",
    "\n",
    "Fasterq-dump does not natively support batch converting of files.\n",
    "\n",
    "There are several ways we can get around this.\n",
    "\n",
    "For instance, one could use loops, or utilize piping.\n",
    "\n",
    "The below code uses a 'for' loop to iterate through all the accession IDs in our accs.txt file.\n",
    "\n",
    "It also adds includes various flags\n",
    "\n",
    "-e for cpu threads\n",
    "-m for maximum memory useage\n",
    "-f to force overwrite\n",
    "-O output directory. \n",
    "\n",
    "This process should take about 35 minutes or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!for x in `cat accs.txt`; do fasterq-dump -f -O data/raw_fastq -e $THREADS -m 4G data/raw_fastq/$x/$x.sra; done\n",
    "\n",
    "##example of how to alternatively do the above process with parallel-fastq-dump using piping\n",
    "#!cat accs.txt | xargs -I {} parallel-fastq-dump -O data/raw_fastq/ --tmpdir . --threads $THREADS --gzip --split-files --sra-id {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, it is good practice to turn .fastq files into .fastq.gz files to save space.\n",
    "\n",
    "In our case, we will actually need to concatenate the fastq files later on, and so will zip after this.\n",
    "\n",
    "The no redundant SRA files can also be deleted to save more space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#find and delete all SRR subfolders in the raw_fastq directory\n",
    "!find data/raw_fastq -type d -name 'SRR*' -exec rm -rf {} \\;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 4: Copy reference transcriptome files that will be used by Salmon using E-Direct\n",
    "\n",
    "Salmon is a tool that aligns RNA-Seq reads to a transcriptome.\n",
    "\n",
    "So we will need a transcriptome reference file.\n",
    "\n",
    "To get one, we can search through the NCBI assembly database, find an assembly, and download transcriptome reference files from that assembly using FTP links.\n",
    "\n",
    "For instance, we will use the <a href='https://www.ncbi.nlm.nih.gov/assembly/GCF_001632805.1'>ASM163280v1</a> refseq assembly, found by searching through the NCBI assembly database. The FTP links can be accessed through the website in various ways, one way is to click the 'FTP directory for RefSeq assembly' link, found under 'Access the data', section.\n",
    "\n",
    "Alternatively, if one were inclined, one could take the less common route and perform this through the NCBI command line tool suite called 'Entrez Direct' (EDirect).\n",
    "\n",
    "This is an intricate and complicated set of tools, with many ways to do any one thing.\n",
    "\n",
    "Below is an example of using an eDirect search query with a refseq identifier to obtain the relevant FTP directory, and then using that to download desired reference files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#parse for the ftp link and download the genome reference fasta file\n",
    "\n",
    "!esearch -db assembly -query GCF_001632805.1 | efetch -format docsum \\\n",
    "| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\\n",
    "| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_genomic.fna.gz \" $0\"/\"$NF\"_genomic.fna.gz\"}' \\\n",
    "| bash\n",
    "\n",
    "#parse for the ftp link and download the gtf reference fasta file\n",
    "\n",
    "!esearch -db assembly -query GCF_001632805.1 | efetch -format docsum \\\n",
    "| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\\n",
    "| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_genomic.gff.gz \" $0\"/\"$NF\"_genomic.gff.gz\"}' \\\n",
    "| bash\n",
    "\n",
    "# parse for the ftp link and download the feature-table reference file \n",
    "# (for later use for merging readcounts with gene names in R code).\n",
    "\n",
    "!esearch -db assembly -query GCF_001632805.1 | efetch -format docsum \\\n",
    "| xtract -pattern DocumentSummary -element FtpPath_RefSeq \\\n",
    "| awk -F\"/\" '{print \"curl -o data/reference/\"$NF\"_feature_table.txt.gz \" $0\"/\"$NF\"_feature_table.txt.gz\"}' \\\n",
    "| bash\n",
    "\n",
    "\n",
    "#unzip the compresseed fasta files\n",
    "\n",
    "!gzip -d data/reference/*.gz --force"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use a tool called gffread to create a transcriptome reference file using the gtf and genome files we downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gffread -w data/reference/GCF_001632805.1_transcriptome_reference.fa -g data/reference/GCF_001632805.1_ASM163280v1_genomic.fna data/reference/GCF_001632805.1_ASM163280v1_genomic.gff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also recommended to include the full genome at the end of the transcriptome reference file, for the purpose of performing a 'decoy-aware' mapping, more information about which can be found in the Salmon documentation.\n",
    "\n",
    "To alert the tool to the presence of this, we will also create a 'decoy file', which salmon needs pointed towards the full genome sequence in our transcriptome reference file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat data/reference/GCF_001632805.1_transcriptome_reference.fa > data/reference/GCF_001632805.1_transcriptome_reference_w_decoy.fa\n",
    "!echo >> data/reference/GCF_001632805.1_transcriptome_reference_w_decoy.fa\n",
    "!cat data/reference/GCF_001632805.1_ASM163280v1_genomic.fna >> data/reference/GCF_001632805.1_transcriptome_reference_w_decoy.fa\n",
    "!echo \"NZ_CP007220.1\" > data/reference/decoys.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: Run FastQC\n",
    "FastQC is an invaluable tool that allows you to evaluate whether there are problems with a set of reads. For example, it will provide a report of whether there is any bias in the sequence composition of the reads.\n",
    "\n",
    "The below code may take around 25 minutes to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run fastqc for forward reads\n",
    "!cat accs.txt | xargs -I {} fastqc \"data/raw_fastq/{}_1.fastq\" -o data/fastqc/\n",
    "#run fastqc for reverse reads\n",
    "!cat accs.txt | xargs -I {} fastqc \"data/raw_fastq/{}_2.fastq\" -o data/fastqc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastqc will output the results in HTML format, as below, for all forward and reverse reads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='./data/fastqc/SRR13349126_1_fastqc.html', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although its best practice to look over them individually, tools like multiqc allow one to quickly look at a summary of the quality reports of the fastq files.\n",
    "\n",
    "For instance, the below table shows which warnings, passes, or failures, from each fastqc report. There are other summaries created as well by multiqc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!multiqc -f data/fastqc/\n",
    "\n",
    "import pandas as pd\n",
    "dframe = pd.read_csv(\"./multiqc_data/multiqc_fastqc.txt\", sep='\\t')\n",
    "display(dframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5.1 Merging our fastq files\n",
    "\n",
    "The following step may not be necessary -- it depends on the study.\n",
    "\n",
    "In our case, if we look at our SRA files:\n",
    "\n",
    "https://trace.ncbi.nlm.nih.gov/Traces/study/?acc=SRP300216&o=acc_s%3Aa\n",
    "\n",
    "We will notice that, although there are 12 SRA files, coming from 12 SRR runs -- Actually, there are only 6 total samples. \n",
    "\n",
    "In such a case it is possible that, for instance, multiple different lanes in a flowcell may have been used for the same sample.\n",
    "\n",
    "In our analysis will be comparing at the sample level. So, we would like to merge the fastq files that, although were created as separate fastq files by the sequencer, actually came from the sample. \n",
    "\n",
    "It is generally easier to do this merging after an initial fastqc report, as it makes it easier to pinpoint errors that may be lane specific.\n",
    "\n",
    "Combining two FASTQ files is a straightforward process. Remember how FASTQ files are formatted, they are a list of readcounts. Consequently, we can simply 'concatenate' or add one fastq file to the bottom of another to create a merged fastq. Note that header information in a single fastq file may now contain different lane information -- however for our downstream processes this is acceptable. Remember if your fastq files are zipped, you will have to unzip them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#example of how to concatenate two of our fastq files from the same experiment.\n",
    "!cat data/raw_fastq/SRR13349122_1.fastq data/raw_fastq/SRR13349123_1.fastq > data/raw_fastq/GSM5004088_1.fastq\n",
    "!cat data/raw_fastq/SRR13349122_2.fastq data/raw_fastq/SRR13349123_2.fastq > data/raw_fastq/GSM5004088_2.fastq\n",
    "#notice we concat the forward read with a forward read, and reverse with reverse.\n",
    "#also note here we are naming it after the GSM, which is the sample experiment ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could manually do the above for all 12 of our runs, and it wouldn't be much work.\n",
    "\n",
    "If you are comfortable doing so, that is the best process to use.\n",
    "\n",
    "As always though, there are ways to automate things. For instance, we could make use of our query code from 3.1.2 to obtain a list of the SRX IDs, and take advantage of our Jupyter's ability to write python to write a loop to iterate and concat our list.\n",
    "\n",
    "Specific to our case, each sample contains two paired-end SRRs. \n",
    "\n",
    "Note; running this step will remove the previous unmerged fastq files in order to save space.\n",
    "\n",
    "This will take about one hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyathena import connect\n",
    "import pandas as pd\n",
    "\n",
    "# Use the correct argument name: s3_staging_dir\n",
    "conn = connect(s3_staging_dir='s3://sra-data-athena/', region_name='us-east-1')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM AwsDataCatalog.srametadata.metadata\n",
    "WHERE organism = 'Mycobacteroides chelonae' \n",
    "AND acc LIKE '%SRR133491%'\n",
    "\"\"\"\n",
    "df = pd.read_sql(\n",
    "    query, conn\n",
    ")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Now get the accession IDs and sample IDs from the created dataframe\n",
    "runs = df['acc'].values\n",
    "samples = list(set(df['sample_name'].values))\n",
    "\n",
    "# Sort them to be in numerical order\n",
    "runs.sort()\n",
    "samples.sort()\n",
    "\n",
    "# Iterate through the samples\n",
    "for index, item in enumerate(samples):\n",
    "    try:\n",
    "        print(f\"Processing sample {samples[index]}...\")\n",
    "\n",
    "        # Concatenate the two SRRs for forward reads\n",
    "        #print(f\"Concatenating forward reads for {samples[index]}...\")\n",
    "        subprocess.run([\"cat\", f\"data/raw_fastq/{runs[index*2]}_1.fastq\", f\"data/raw_fastq/{runs[index*2+1]}_1.fastq\", \">\", f\"data/raw_fastq/{samples[index]}_1.fastq\"], check=True)\n",
    "\n",
    "        # Delete the original fastq files for forward reads\n",
    "        #print(f\"Deleting original forward reads for {samples[index]}...\")\n",
    "        subprocess.run([\"rm\", f\"data/raw_fastq/{runs[index*2]}_1.fastq\"], check=True)\n",
    "        subprocess.run([\"rm\", f\"data/raw_fastq/{runs[index*2+1]}_1.fastq\"], check=True)\n",
    "\n",
    "        # Zip the merged forward fastq file\n",
    "        #print(f\"Zipping forward reads for {samples[index]}...\")\n",
    "        subprocess.run([\"gzip\", f\"data/raw_fastq/{samples[index]}_1.fastq\"], check=True)\n",
    "\n",
    "        # Concatenate the two SRRs for reverse reads\n",
    "        #print(f\"Concatenating reverse reads for {samples[index]}...\")\n",
    "        subprocess.run([\"cat\", f\"data/raw_fastq/{runs[index*2]}_2.fastq\", f\"data/raw_fastq/{runs[index*2+1]}_2.fastq\", \">\", f\"data/raw_fastq/{samples[index]}_2.fastq\"], check=True)\n",
    "\n",
    "        # Delete the original fastq files for reverse reads\n",
    "        #print(f\"Deleting original reverse reads for {samples[index]}...\")\n",
    "        subprocess.run([\"rm\", f\"data/raw_fastq/{runs[index*2]}_2.fastq\"], check=True)\n",
    "        subprocess.run([\"rm\", f\"data/raw_fastq/{runs[index*2+1]}_2.fastq\"], check=True)\n",
    "\n",
    "        # Zip the merged reverse fastq file\n",
    "        #print(f\"Zipping reverse reads for {samples[index]}...\")\n",
    "        subprocess.run([\"gzip\", f\"data/raw_fastq/{samples[index]}_2.fastq\"], check=True)\n",
    "\n",
    "        print(f\"Processing complete for sample {samples[index]}.\\n\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error processing sample {samples[index]}: {e}\")\n",
    "        # Optionally, you can exit or skip to the next sample\n",
    "        continue\n",
    "\n",
    "print(\"All samples processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#since our files will now be samples, not SRRs we can write a new text file to use for downstream batch processes.\n",
    "#we can use the DF we made in the previous cell.\n",
    "with open('samples.txt', 'w') as f:\n",
    "    df = df.sort_values(by='sample_name', ascending=True)\n",
    "    samples = df['sample_name'].unique()\n",
    "    samples = '\\n'.join(map(str, samples))\n",
    "    f.write(samples)\n",
    "    \n",
    "!cat samples.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5.3: Copy data file for Trimmomatic\n",
    "\n",
    "One of trimmomatics functions is to trim sequence machine specific adapter sequences. These are usually within the trimmomatic installation directory in a folder called adapters.\n",
    "\n",
    "Directories of packages within conda installations can be confusing, so in the case of using conda with trimmomatic, it may be easier to simply download or create a file with the relevant adapter sequencecs and store it in an easy to find directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil -m cp -r gs://nigms-sandbox/me-inbre-rnaseq-pipelinev2/config/TruSeq3-PE.fa .\n",
    "!head TruSeq3-PE.fa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 6: Run Trimmomatic\n",
    "Trimmomatic will trim off any adapter sequences or low quality sequence it detects in the FASTQ files.\n",
    "\n",
    "Using piping and our original list, it is possible to queue up a batch run of trimmomatic for all our files, note that this is a different way to run a loop compared with what we did before.\n",
    "\n",
    "The below code may take approximately 35 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat samples.txt | xargs -I {} trimmomatic PE -threads $THREADS 'data/raw_fastq/{}_1.fastq.gz' 'data/raw_fastq/{}_2.fastq.gz' 'data/trimmed/{}_1_trimmed.fastq.gz' 'data/trimmed/{}_1_trimmed_unpaired.fastq.gz' 'data/trimmed/{}_2_trimmed.fastq.gz' 'data/trimmed/{}_2_trimmed_unpaired.fastq.gz' ILLUMINACLIP:TruSeq3-PE.fa:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 MINLEN:36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### STEP 7 (Optional): Run FastQC\n",
    "\n",
    "It's best practice to run FastQC after trimming. However, you may decide to run FastQC only once, before or after trimming.\n",
    "\n",
    "We will proceed with only the forward reads -- this is because, looking at trimmomatic, there were very few 'orphaned' reads. That is to say, most forward and reverse reads were successfully paired together. Because we are just trying to map to a transcriptome, the read lengths of the forward reads alone, in this case, around 50~ basepairs, should be sufficient.\n",
    "\n",
    "The below code may take around 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat samples.txt | xargs -I {} fastqc \"data/trimmed/{}_1_trimmed.fastq.gz\" -o data/fastqc_samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: Run MultiQC\n",
    "MultiQC reads in the FastQC reports and generate a compiled report for all the analyzed FASTQ files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!multiqc -f data/fastqc_samples/\n",
    "!multiqc -f -o data/multiqc_samples/ data/fastqc_samples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 9: Index the Transcriptome so that Trimmed Reads Can Be Mapped Using Salmon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run salmon we must specify the reference transcriptome, and the folder of our created index.\n",
    "\n",
    "Note here, -i does not mean input, but the folder where the index will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!salmon index -t data/reference/GCF_001632805.1_transcriptome_reference_w_decoy.fa -p $THREADS -i data/reference/transcriptome_index --decoys data/reference/decoys.txt -k 31 --keepDuplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 10: Run Salmon to Map Reads to Transcripts and Quantify Expression Levels\n",
    "Salmon aligns the trimmed reads to the reference transcriptome and generates the read counts per transcript. In this analysis, each gene has a single transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat samples.txt | xargs -I {} salmon quant -i data/reference/transcriptome_index -l SR -r \"data/trimmed/{}_1_trimmed.fastq.gz\" -p 8 --validateMappings -o \"data/quants/{}_quant\" > dump.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 11: Report the top 10 most highly expressed genes in the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in each wild-type sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head data/quants/SRR13349122_quant/quant.sf -n 1\n",
    "!sort -nrk 4,4 data/quants/GSM5004088_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/GSM5004089_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/GSM5004090_quant/quant.sf | head -10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 most highly expressed genes in the double lysogen samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head data/quants/SRR13349122_quant/quant.sf -n 1\n",
    "!sort -nrk 4,4 data/quants/GSM5004091_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/GSM5004092_quant/quant.sf | head -10\n",
    "!sort -nrk 4,4 data/quants/GSM5004093_quant/quant.sf | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 12: Report the expression of a putative acyl-ACP desaturase (BB28_RS16545) that was downregulated in the double lysogen relative to wild-type\n",
    "A acyl-transferase was reported to be downregulated in the double lysogen as shown in the table of the top 20 upregulated and downregulated genes from the paper describing the study.\n",
    "![RNA-Seq workflow](images/table-cushman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the wild-type sample. The fields in the Salmon `quant.sf` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!grep 'BB28_RS16545' data/quants/GSM5004088_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/GSM5004089_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/GSM5004090_quant/quant.sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `grep` to report the expression in the double lysogen sample. The fields in the Salmon `quant.sf` file are as follows. The level of expression is reported in the Transcripts Per Million (`TPM`) and number of reads (`NumReads`) fields:  \n",
    "`Name    Length  EffectiveLength TPM     NumReads`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!grep 'BB28_RS16545' data/quants/GSM5004091_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/GSM5004092_quant/quant.sf\n",
    "!grep 'BB28_RS16545' data/quants/GSM5004093_quant/quant.sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 12: Combine Genecounts to a Single Genecount File\n",
    "Commonly, the readcounts for each sample are combined into a single table, where the rows contain the gene ID, and the columns identify the sample.\n",
    "\n",
    "As before, this can be done in many ways. The quantmerge function outputs a table.\n",
    "\n",
    "However, it is common for readcount tables to have sample headers for the columns, which this does not have.\n",
    "\n",
    "So you could manually add those headers in different ways, for instance using a spreadsheet editor, or using a shell command like sed as below to insert a line at the start of table with the relevant tab-seperated headers.\n",
    "\n",
    "You could also use sed to remove 'gene' or 'rna' prefixes from the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##first merge salmon files by number of reads.\n",
    "!salmon quantmerge --column numreads --quants data/quants/*_quant -o data/quants/merged_quants.txt\n",
    "##optinally we can rename the columns\n",
    "!sed -i \"1s/.*/Name\\tGSM5004088\\tGSM5004089\\tGSM5004090\\tGSM5004091\\tGSM5004092\\tGSM5004093/\" data/quants/merged_quants.txt\n",
    "\n",
    "##for further formatting, it may be easier in our r-code to later merge\n",
    "##if we remove the gene- and rna- prefix\n",
    "!sed -i \"s/gene-//\" data/quants/merged_quants.txt\n",
    "!sed -i \"s/rna-//\" data/quants/merged_quants.txt\n",
    "\n",
    "print(\"An example of a combined genecount outputfile.\")\n",
    "!head data/quants/merged_quants.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a name=\"workflow\">Additional Workflows</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have read counts per gene, feel free to explore the R workflow which creates plots and analyses using these readcount files, or try other alternate workflows for creating read count files, such as using snakemake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Workflow One:](Tutorial_1.ipynb) A short introduction to downloading and mapping sequences to a transcriptome using Trimmomatic and Salmon. Here is a link to the YouTube video demonstrating the tutorial: <https://youtu.be/ChGfBR4do_Y>.\n",
    "\n",
    "[Workflow One (Extended):](Tutorial_1B_Extended.ipynb) An extended version of workflow one. Once you have got your feet wet, you can retry workflow one with this extended version that covers the entire dataset, and includes elaboration such as using SRA tools for sequence downloading, and examples of running batches of fastq files through the pipeline. This workflow may take around an hour to run.\n",
    "\n",
    "[Workflow One (Using Snakemake):](Tutorial_2_Snakemake.ipynb) Using snakemake to run workflow one.\n",
    "\n",
    "[Workflow Two (DEG Analysis):](Tutorial_3_DEG_Analysis.ipynb) Using Deseq2 and R to conduct clustering and differential gene expression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNA-Seq workflow](images/RNA-Seq_Notebook_Homepage.png)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m112"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
